---
title: How tall am I? A Bayesian exploration using the Julia language.
author: Denis Vlašiček
date: `j using Dates; Dates.Date(Dates.now())`
weave_options:
    echo: true
    dpi: 300
    out_width: "800px"
    out_height: "600px"
---

Being a tall person, I often get asked how tall I am when I meet new people.
For years, I have been claiming that I am 196 centimeters tall. However, a few
months ago I realized that this is a somewhat old piece of information; while I
did not believe that my actual height has changed, I thought it possible that I
misremembered. That is why I have decided to collect some data and conduct a
simple Bayesian estimation of my height, using the Julia language.
In what follows, I try to estimate my height, exploring the Julia language along
the way.

The code and the data are available in the GitHub repository
[here](https://github.com/vdeni/how-tall/).

## Setting up the environment

In order to conduct the planned analysis, we will load some Julia modules. We
will use `{CSV}` to read in the data, which are stored in a comma separated
values (csv) file. We will store that data as a `{DataFrames}` `DataFrame`
object, which is similar to the `data.frame` objects in the R language.

We will use some functions from `{Dates}` to determine the days of the week
on which the measrurements were taken, and use `{CategoricalArrays}` to declare
an ordered categorical variable holding the days of the week.

We will use `{Turing}` and `{MCMCChains}` to fit and handle the Bayesian
modelling.

Finally, we will use `{Plots}` and `{StatsPlots}` for various visualization
functions.

```julia
# load packages
using CSV
using DataFrames
using Dates
using CategoricalArrays
using Turing
using MCMCChains
using Plots
using StatsPlots

# I chose the GR backend for plotting. See Plots.jl documentation for more
# information. The semicolon is just to suppress the printing of the function's
# output.
Plots.GRBackend();
```

## Loading the data

As mentioned earlier, the data is stored in a CSV file, which I'll read in using
the `{CSV}` module. The read data will be dumped into a `DataFrame`, as
indicated by the second argument to the `CSV.read` function. I am also using the
`joinpath` function to supply the path to the data file in order to make the
code portable across different operating systems.

```julia
# load data into DataFrame
d = CSV.read(joinpath("..",
                      "analyses",
                      "data",
                      "height_d.csv"),
             DataFrames.DataFrame)

show(d)
```

We can see that the data frame has three columns: the date of the measurement
(ISO formatted), the hour of the measurement (in the 24-hour format), and the
recorded height (in centimeters). We can also see the types of the values stored
in the columns (`Date`, `Int64` and `Float64`), and that there are 30 rows in
total.

## Graphing the data

Now that the data is loaded, I'll make some plots to inspect it. First, I'll try
plotting the distribution of height measurements across the days of the week. In
order to do that, I will first get the day of the week for each of the recorded
dates. I'll do that using the `dayname` function from the `Dates` package. I
will also store the data as a categorical variable, i.e. as variable of the type
`CategoricalArray`.

```julia
# get day of the week for each date in the dataset
d.weekday = CategoricalArrays.CategoricalArray(Dates.dayname.(d.date);
                                               ordered = true)

# set the ordering of the levels of the categorical variable
CategoricalArrays.levels!(d.weekday,
                          ["Monday", "Tuesday", "Wednesday", "Thursday",
                           "Friday", "Saturday", "Sunday"])

show(d)
```

The data frame now has a new variable, `weekday`, which I'll use for plotting.
In the code below, I'm using a multiline comment (`#= ... =#`) to split the
`@df` macro from the `scatter` function call. The two could be on the same line,
but I think the code is cleaner this way. Note that the `@df` macro and the
function call cannot be split by a simple line break; that's why I'm using a
multiline comment (the idea came from
[this](https://github.com/JuliaLang/julia/issues/27533#issuecomment-396381213)
comment).

```julia
# set some variables used for plot aesthetics
marker_color = Plots.palette(:viridis, 3)[2]
line_color = Plots.palette(:viridis, 3)[2]

# make scatterplot of days of the week against height measurements.
p_height_by_day = StatsPlots.@df d #=
    =# Plots.scatter(:weekday,
                     :height_cm;
                     yticks = 194:200,
                     ylims = (194.5, 199.5),
                     # here, I've extended the limits on the x-axis somewhat
                     # in order to put some padding between the y-axis and the
                     # points for Monday measurements
                     xlims = (0.4, 7.5),
                     xticks = 1:7,
                     legend = false,
                     color = marker_color,
                     dpi = 300,
                     size = (800, 600),
                     xlabel = "Day of the week",
                     ylabel = "Height in centimeters",
                     markeralpha = .7,
                     markerstrokewidth = 1,
                     gridstyle = :dash)
```

The scatterplot seems to imply that there's no correlation between the days of
the week and height. Of course, no such correlation should be expected.

However, there should be *some* correlation between the time at which a
measurement was taken, and its amount -- higher values should be observed
earlier in the day, and lower values later in the day. Therefore, I will also
plot the hours at which the measurements were taken against the measured
heights. I will first filter out a single measurement that was taken at
midnight, in order to get a prettier plot; the single measurement taken at that
time should not be very informative.

```julia
# plot height by time of day

# filter out the single measurement that was taken at midnight. the x -> ...
# syntax is used to creat an anonymous function, which is used for filtering
d_p_by_hour = DataFrames.filter(x -> x.time_hours > 0,
                                d)

p_height_by_hour = StatsPlots.@df d_p_by_hour #=
    =# Plots.scatter(:time_hours,
                     :height_cm;
                     yticks = 194:199,
                     ylims = (194.5, 199.5),
                     xticks = 10:2:20,
                     # again, somewhat extended xlims for visual clarity
                     xlims = (9, 21),
                     legend = false,
                     color = marker_color,
                     dpi = 300,
                     size = (800, 600),
                     xlabel = "Time of day (hours)",
                     ylabel = "Height in centimeters",
                     markeralpha = .7,
                     markerstrokewidth = 1,
                     gridstyle = :dash)

```

Indeed, the plotted data seem to imply that such a correlation exists.
Finally, I'll plot the density of the height measurements.

```julia
# plot height distribution
p_height_distr = StatsPlots.@df d #=
    =# StatsPlots.density(:height_cm;
                          dpi = 300,
                          size = (800, 600),
                          fillcolor = Plots.palette(:viridis, 2)[1],
                          fillalpha = .5,
                          fillrange = 0,
                          color = :black,
                          yaxis = nothing,
                          xlabel = "Height in centimeters",
                          grid = false,
                          yshowaxis = false,
                          legend = :none)
```

## Bayesian estimation

After this cursory glance at the data, I'm turning to Bayesian modelling in
order to estimate my height. I've decided to model the data as coming from a
normal distribution with parameters $\mu$ and $\sigma$. I've also assumed that
the $\mu$ parameter comes from a $Normal(196, .75)$ distribution. I've chosen
this distribution because I think it adequately captures my prior belief about
my height -- I believe that I'm 196 cm tall, but leave (a little) space for
values above and below; the .1, .5 and .9 quantiles of this distribution are
`j show(round.(quantile(Turing.Normal(196, .75), [.1, .5, .9]), digits = 3))`.

I've chosen an $Exponential(1)$ distribution as the prior for the standard
deviation of the data distribution; this exponential distribution's .1, .5 and
.9 quantiles are
`j show(round.(quantile(Turing.Exponential(1), [.1, .5, .9]), digits = 3))`. I
think this adequately represents my belief that the variation of the height
measurements should not be too high. Note that these priors were chosen before
plotting the data, but during data collection.

Now, let's code up the Bayesian model in `{Turing}`. This is fairly easy, given
that this is a simple model.

```julia
# build height estimation model
@model function m_height(height)
    # set the priors for the μ and σ parameters
    μ ~ Normal(196, .75)
    σ ~ Exponential(1)

    # set likelihood for the data
    for i in 1:length(height)
        height[i] ~ Normal(μ, σ)
    end
end;
```

To sample from the model, we use the `sample` function, with the Bayesian model
as the first argument. In the chunk below, I'm telling Julia to use the `NUTS`
sampler (the same one that Stan is using), to draw 3000 post-warmup samples for
each chain, and to run 6 parallel chains. For `MCMCThreads()` to work, Julia has
to be started with the `-t X` option set, where `X` is the number of CPU cores
available to Julia.

```julia
# sample
chains = Turing.sample(m_height(d.height_cm),
                       # sampler; 1500 warmup steps, .8 target acceptance rate
                       NUTS(1500, .80),
                       # how to do parallel computation; check the docs
                       MCMCThreads(),
                       # number of post-warmup iterations
                       3000,
                       # number of chains
                       6;
                       # do not keep warmup samples
                       discard_adapt = true)

d_summary = MCMCChains.summarize(chains)
display(d_summary[:, [:mean, :std, :ess]])

d_quantiles = MCMCChains.quantile(chains)
display(d_quantiles)
```

According to the results, the mean of the $\mu$ estimates is
`j round(d_summary[1, :mean]; digits = 2)`, with 95% of the estimates falling
between `j round(d_quantiles[:μ, Symbol("2.5%")]; digits = 2)` and
`j round(d_quantiles[:μ, Symbol("97.5%")]; digits = 2)`. So, based on the data
and the prior, I conclude that I'm a bit taller than I initially thought.

## Model diagnostics

Finally, after fitting the model, we'll take a look at some Bayesian model
diagnostics.Bayesian model diagnostics. First, we'll look at the trace plots and
density plots of the Markov chains, to see whether they mix well, converge, and
efficiently explore the parameter space.

```julia
# check trace and posterior density plots
Plots.plot(chains;
           size = (800, 600))
```

The plots on the left show the parameters values each chain explored in each
step of Monte Carlo sampling. The chains seem to be well mixed, and seem not to
be stuck around a narrow range of values. This is good. The plots on the right
show the densities of the values sampled in each chain. The overlap of the
distributions on these plots also implies that the chains have converged, and
that they are producing representative samples from the posterior (see chapter
7 in John Kruschke's *Doing Bayesian Data Analysis* for a friendly explanation
of various MCMC diagnostics).

We'll also check the $\hat{R}$ statistic and the effective sample size (ESS),
which are provided by the `summarize` function, and present in our `d_summary`
table.

```julia
display(d_summary[:, [:ess, :rhat]])
```

Values of $\hat{R}$ close to 1 indicate that the chains have converged, which is
the case for this model. The effective sample size tries to give an estimate of
the amount of independent information present in the chains. The estimated
effective sample sizes for this model should be sufficient for our purposes.

Finally, we'll make a posterior predictive check. That is, we'll try simulating
new height measurements from the fitted model, and compare them to the data.
If I understood the `{Turing}` documentation correctly, this can easily be done 
by using the `predict` function, and calling the Bayesian model with a vector of
`missing` values as data. This is done in the chunk below:

```julia
# make posterior predictive check
ppc = Turing.predict(m_height(Vector{Union{Missing, Number}}(missing,
                                                             nrow(d))),
                     chains)
```

Now that we have simulated data, we can plot it against the observed data.
First, we'll extract the draws into a `DataFrame`, and remove the brackets from
the column names.

```julia
# extract draws
d_ppc = DataFrames.DataFrame(ppc)

# fix column names
newnames = replace.(names(d_ppc),
                    "[" => "_") |>
    x -> replace.(x,
                  "]" => "")

d_ppc = DataFrames.rename(d_ppc,
                          newnames)
```

Next, we'll extract the simulated data from 1500 randomly chosen iterations.
I'll reshape the `DataFrame` into the long format, for easier plotting; this is
done with the `stack` function.

```julia
# randomly sample 1500 simulated height vectors (each with 30 simulated
# measurements)
v_iters = sample(1:nrow(d_ppc),
                 1500;
                 replace = false)

d_ppc_plot = d_ppc[v_iters, :]

# add id variable; this will be used to group the values when plotting
d_ppc_plot.id = 1:nrow(d_ppc_plot)

# reshape data for plotting
d_ppc_plot = DataFrames.stack(d_ppc_plot,
                              r"height_";
                              variable_name = "rep_id",
                              value_name = "rep_height")
```

Now that the data is ready, I'll plot the distributions of the simulated values
and the distribution of the observed values.

```julia
# plot posterior predictions
p_ppc = StatsPlots.@df d_ppc_plot #=
    =# Plots.density(:rep_height;
                     dpi = 300,
                     size = (800, 600),
                     group = :id,
                     legend = :topright,
                     linecolor = :black,
                     linewidth = 1,
                     linealpha = .075,
                     yaxis = nothing,
                     xlabel = "Height in centimeters",
                     grid = false,
                     yshowaxis = false,
                     # this is creates a row-vector with one entry, and 1499
                     # empty strings. I'm doing this so that the legend doesn't
                     # hold 1500 entries which say "Posterior predictions".
                     # this way, there's a single entry for the 1500 densities
                     label = ["Posterior predictions" #=
                              =# repeat([""], 1, 1499)])

Plots.density!(p_ppc,
               d.height_cm;
               linewidth = 2,
               linecolor = line_color,
               fillrange = 0,
               fill = line_color,
               fillalpha = .5,
               label = "Data")

Plots.vline!(p_ppc,
             [196];
             color = :red,
             label = nothing)

Plots.vline!(p_ppc,
             [d_summary[:μ, :mean]];
             color = :red,
             linestyle = :dash,
             label = nothing)
```

The thin black lines are the posterior simulations, and the aquamarine
distribution is the observed data. We can see that there is considerable
variation in the distributions produced by the simulations.
The solid red line is set at 196 cm, and the dashed red line is set at the mean
of the posterior $\mu$ estimates.
